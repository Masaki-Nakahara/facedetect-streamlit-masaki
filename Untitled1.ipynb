{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "77b58bf5-d6c8-4aac-882c-d66d1b8cb9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fbfb6db4-9999-4fa4-9a44-0bdabbd616fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_key = '75c6dbfc756c4018afcb647884be3edd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f07e9264-ec14-4941-8b59-9e342992e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert subscription_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "26d56377-65db-4394-ae21-a19d9a06cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_api_url = 'https://20220708masaki.cognitiveservices.azure.com/face/v1.0/detect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e939db5e-fc95-4468-8602-285691f03197",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('sample1.JPG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2c353a22-8c7b-4eea-bc50-e9d054bf0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample1.JPG', 'rb') as f:\n",
    "    binary_img = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ec0aceee-e767-4e07-917b-9b66dbef86c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Content-Type': 'application/octet-stream',\n",
    "    'Ocp-Apim-Subscription-Key': subscription_key\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'returnFaceId': 'true',\n",
    "    'returnFaceAttributes': 'age,gender,headPose,smile,facialHair,glasses,emotion,hair,makeup,occlusion,accessories,blur,exposure,noise'\n",
    "}\n",
    "\n",
    "res = requests.post(face_api_url, params=params, headers=headers, data = binary_img)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "074bd2af-2d6a-43e4-9c7f-351778380696",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "98e2ed7d-42ed-4ee9-9a72-3de69444d26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "97a34441-1a78-42ec-ae42-d1130e4c9edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': {'code': 'InvalidRequest',\n",
       "  'message': 'Invalid request has been sent.',\n",
       "  'innererror': {'code': 'UnsupportedFeature',\n",
       "   'message': 'Feature is not supported. Please apply for access at https://aka.ms/facerecognition'}}}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = res.json()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb574a9-9bd9-4b5f-8ffa-42251950377f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cognitive_face in /Users/nakaharamizuki/.pyenv/versions/3.9.0/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: requests in /Users/nakaharamizuki/.pyenv/versions/3.9.0/lib/python3.9/site-packages (from cognitive_face) (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nakaharamizuki/.pyenv/versions/3.9.0/lib/python3.9/site-packages (from requests->cognitive_face) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/nakaharamizuki/.pyenv/versions/3.9.0/lib/python3.9/site-packages (from requests->cognitive_face) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nakaharamizuki/.pyenv/versions/3.9.0/lib/python3.9/site-packages (from requests->cognitive_face) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/nakaharamizuki/.pyenv/versions/3.9.0/lib/python3.9/site-packages (from requests->cognitive_face) (2.0.11)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/Users/nakaharamizuki/.pyenv/versions/3.9.0/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cognitive_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f313b6b-4ce9-40ef-8b81-a95ee5a03d70",
   "metadata": {},
   "outputs": [
    {
     "ename": "CognitiveFaceException",
     "evalue": "Error when calling Cognitive Face API:\n\tstatus_code: 404\n\tcode: 404\n\tmessage: Resource not found\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCognitiveFaceException\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m CF\u001b[38;5;241m.\u001b[39mBaseUrl\u001b[38;5;241m.\u001b[39mset(BASE_URL)\n\u001b[1;32m      8\u001b[0m img_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample1.JPG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mCF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memotion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(faces)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/cognitive_face/face.py:40\u001b[0m, in \u001b[0;36mdetect\u001b[0;34m(image, face_id, landmarks, attributes)\u001b[0m\n\u001b[1;32m     33\u001b[0m headers, data, json \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mparse_image(image)\n\u001b[1;32m     34\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturnFaceId\u001b[39m\u001b[38;5;124m'\u001b[39m: face_id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturnFaceLandmarks\u001b[39m\u001b[38;5;124m'\u001b[39m: landmarks \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturnFaceAttributes\u001b[39m\u001b[38;5;124m'\u001b[39m: attributes,\n\u001b[1;32m     38\u001b[0m }\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/cognitive_face/util.py:103\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, data, json, headers, params)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CognitiveFaceException(response\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    102\u001b[0m                                      response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CognitiveFaceException(response\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    104\u001b[0m                                  error_msg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    105\u001b[0m                                  error_msg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Prevent `response.json()` complains about empty response.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext:\n",
      "\u001b[0;31mCognitiveFaceException\u001b[0m: Error when calling Cognitive Face API:\n\tstatus_code: 404\n\tcode: 404\n\tmessage: Resource not found\n"
     ]
    }
   ],
   "source": [
    "import cognitive_face as CF\n",
    "KEY = '75c6dbfc756c4018afcb647884be3edd'\n",
    "BASEï¼¿URL = 'https://20220708masaki.cognitiveservices.azure.com/face/v1.0/detect'\n",
    "\n",
    "CF.Key.set(KEY)\n",
    "CF.BaseUrl.set(BASE_URL)\n",
    "\n",
    "img_url = \"sample1.JPG\"\n",
    "faces = CF.face.detect(img_url, attributes='emotion')\n",
    "\n",
    "print(faces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e7613-017d-4441-99ee-866281bf7c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
